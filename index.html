<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

</head>
<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://manifoldrg.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Related Projects
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/eihli/mugato">
            Î¼Gato
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>



  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MultiNet: A Generalist Benchmark for Vision-Language & Action models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">This is a collaborative effort between <a href="https://www.manifoldrg.com/">Manifold Research</a> and <a href="https://metarch.ai/">Metarch.ai</a></span>
              <span class="author-block">This work is sponsored by <a href="https://metarch.ai/">Metarch.ai</a></span>
            </div>

            <div class="publication-links">
              <!-- Multinet v0.1 page -->
              <span class="link-block">
                <a href="static/pages/Multinetv01.html" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-robot"></i>
                  </span>
                  <span>Multinetv0.1</span>
                </a>
              </span>

              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary PDF link -->
              <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Dataset Spec</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/YOUR REPO HERE" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- Link to VLM-Action framework -->
              <span class="link-block">
                <a href="https://github.com/ManifoldRG/MultiNet/tree/main/src/modules" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-code"></i>
                  </span>
                  <span>GenESIS framework</span>
                </a>
              </span>

              <!-- Link to VLM-Action framework -->
              <span class="link-block">
                <a href="https://github.com/eihli/mugato" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-code"></i>
                  </span>
                  <span>Î¼Gato</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Introduction</h2>
            <p>
              Introducing MultiNet: a comprehensive benchmark we are building to evaluate Vision-Language and Multimodal Action models on their generalist capabilities. With Multinet, we aim to provide:
            </p>
            <ul>
              <li>The largest collection of open-source datasets for evaluating Vision-Language and Multimodal Action models</li>
              <li>Open-source software infrastructure for downloading, managing, and utilizing the datasets mentioned above</li>
              <li>Open-source code to translate RL and Robotics datasets of various formats from various sources into a unified Tensorflow Datasets format</li>
              <li>A general framework for mapping VLMs to other modality classes, with particular emphasis on action spaces</li>
            </ul>
            <p>
              In this <a href="static/pages/Multinetv01.html">initial release</a>, we present the results of evaluating SOTA VLM, VLA and generalist model on 20 OpenX datasets. We aim to quickly iterate and expand our evaluation results to more datasets within Multinet.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <style>
    .news-section {
      padding: 20px 0;
      max-height: 400px;
      overflow-y: auto;
    }
  
    .news-title {
      font-size: 24px;
      font-weight: bold;
      margin-bottom: 10px;
    }
  
    .news-container {
      display: flex;
      flex-direction: column;
      gap: 10px;
      background-color: #f5f5f5;
      padding: 20px;
      border-radius: 5px;
    }
  
    .news-item {
      display: flex;
      align-items: center;
      padding: 10px;
      background-color: white;
      border-radius: 5px;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
  
    .news-icon {
      font-size: 24px;
      margin-right: 10px;
    }
  
    .news-content {
      flex-grow: 1;
    }
  
    .news-content a {
      color: blue;
      text-decoration: none;
    }
  
    .news-content a:hover {
      text-decoration: underline;
    }
  </style>

  <section class="news-section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">News</h2>
            <div class="news-container">
              <div class="news-item">
                <div class="news-icon">ðŸŽ‰</div>
                <div class="news-content">
                  <p><strong>Multinet v0.1!</strong> How generalist are today's VLMs and VLAs? Read more on our <a href="static/pages/Multinetv01.html">Release page</a></p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Motivation</h2>
            <ul>
              <li>Deepmind's <a href="https://deepmind.google/discover/blog/a-generalist-agent/">GATO</a> showed the first glimpse of what a generalist model could look like, and the capabilities it could unlock.
                <ul>
                  <li>Dataset composition - Gato is trained on a large number of datasets comprising agent experience in both simulated and real world environments, as well as a variety of natural language and image datasets. However, most of GATO's datasets are internal and closed-source, and the model was never released - both OS or API</li>
                </ul> 
              <li>The emerging class of Vision-Language-Action models (<a href="https://openvla.github.io/">OpenVLA</a>, <a href="https://octo-models.github.io/">Octo</a>) display interesting capabilities of taking vision-language-grounded action. However, they are trained for a niche set of robotics tasks, very nascent, and might not be great at challenging vision-language understanding/generation tasks</li>
              <li>In order to truly build something generalist that can work on a myriad of tasks, in a variety of environments, a model/system needs to be trained on a combination of multiple modalities. It should have the ability to understand vision-language input, and generate meaningful outputs. It should have the ability to generate actions for different tasks in a variety of environments where the observation and action spaces can either be continuous or discrete. This will be vision-language-action in the truest sense - good at each of these modalities, as well as at tasks that involve combinations of these modalities - this is what typically occurs in the real world. </li>
            </ul>
            <p>
              There doesn't exist a large, truly open-source, generalist dataset that can be used for pre-training, fine-tuning, and eval. This is the main reason the Multinet effort started.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  

  <style>
    table {
      width: 100%;
      border-collapse: collapse;
      font-family: "Google Sans", "Noto Sans", "Castoro", sans-serif;
    }
    
    th, td {
      padding: 12px 16px;
      text-align: left;
      border-bottom: 1px solid #ddd;
    }
    
    th {
      background-color: #f2f2f2;
      font-weight: bold;
    }
    
    tr:hover {
      background-color: #f5f5f5;
    }
    
    caption {
      font-style: italic;
      margin-top: 8px;
    }
  </style>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-justified">
          <h2 class="title is-3 has-text-centered">Dataset Coverage and Analysis</h2>
          <p>
            Multinet brings a diverse set of datasets which can be used to train models that will have high vision-language 
            association knowledge, language understanding and generation abilities, and prowess in reward-based action
            trajectories in a variety of environments and tasks. Additionally, it can be used to evaluate SOTA VLMs and
            VLAs to showcase where they are lacking and what work needs to be done to build the next generation of truly
            generalist models.
          </p>
            
            <table>
              <thead>
                <tr>
                  <th>Dataset</th>
                  <th>Task/Data type</th>
                  <th>Modality</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>OBELICS</td>
                  <td>Interleaved Image-Text</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>COYO-700M</td>
                  <td>Image-Text pairs</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>MS COCO</td>
                  <td>Object detection, segmentation, key-point detection, captioning</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>Conceptual Captions</td>
                  <td>Image Captioning</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>A-OKVQA</td>
                  <td>Visual Question Answering</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>VQA-v2</td>
                  <td>Visual Question Answering</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>Datacomp-1B</td>
                  <td>Image-Text pairs</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>Fineweb-edu</td>
                  <td>High quality text corpus</td>
                  <td>Language</td>
                </tr>
              </tbody>
            </table>
            
            <p style="text-align: center; font-weight: bold;">Categorization of Vision-Language and Language datasets that can be used for training, fine-tuning, and some of them for evaluation</p>

            
            <table>
              <thead>
                <tr>
                  <th>Dataset</th>
                  <th>Task/Data type</th>
                  <th>Modality</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Flickr30k</td>
                  <td>Image Captioning</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>TextVQA</td>
                  <td>Visual Question Answering</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>VizWiz</td>
                  <td>Visual Question Answering</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>WinoGAViL</td>
                  <td>Vision-based Commonsense Reasoning</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>ImageNet-R</td>
                  <td>Image-Text Pairs</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>ObjectNet</td>
                  <td>Image-Text Pairs</td>
                  <td>Vision-Language</td>
                </tr>
                <tr>
                  <td>Hellaswag</td>
                  <td>Commonsense Reasoning</td>
                  <td>Language</td>
                </tr>
                <tr>
                  <td>ARC</td>
                  <td>Complex Reasoning and Knowledge Application</td>
                  <td>Language</td>
                </tr>
                <tr>
                  <td>CommonsenseQA</td>
                  <td>Commonsense Reasoning</td>
                  <td>Language</td>
                </tr>
                <tr>
                  <td>MMLU</td>
                  <td>Knowledge-intensive Question Answering</td>
                  <td>Language</td>
                </tr>
              </tbody>
            </table>

            <p style="text-align: center; font-weight: bold;">Categorization of Vision-Language and Language datasets that are purely for evaluation</p>

            <table>
              <thead>
                <tr>
                  <th>Dataset</th>
                  <th>Task/Data type</th>
                  <th>Control Category</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>DM Lab</td>
                  <td>Teach RL Agents 3D vision</td>
                  <td>Navigation-based control</td>
                </tr>
                <tr>
                  <td>DM Control Suite</td>
                  <td>Physics-based simulation environments</td>
                  <td>Locomotion-based control</td>
                </tr>
                <tr>
                  <td>ALE Atari</td>
                  <td>Atari games</td>
                  <td>Game-based control</td>
                </tr>
                <tr>
                  <td>Baby AI</td>
                  <td>Language-grounded navigation</td>
                  <td>Navigation-based control</td>
                </tr>
                <tr>
                  <td>MuJoCo</td>
                  <td>Multi-joint dynamics</td>
                  <td>Locomotion-based control</td>
                </tr>
                <tr>
                  <td>V-D4RL</td>
                  <td>Pixel-based analogues of DM Control Suite</td>
                  <td>Locomotion-based control</td>
                </tr>
                <tr>
                  <td>Procgen</td>
                  <td>Procedurally generated Atari-like environments</td>
                  <td>Game-based control</td>
                </tr>
                <tr>
                  <td>Open X Embodiment</td>
                  <td>Real-world Robotics tasks</td>
                  <td>Manipulation-based control and Locomotion-based control</td>
                </tr>
                <tr>
                  <td>LocoMuJoCo</td>
                  <td>Imitation learning for locomotion</td>
                  <td>Locomotion-based control</td>
                </tr>
              </tbody>
            </table>
            
            <p style="text-align: center; font-weight: bold;">Categorization of control datasets</p>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Benchmark</h2>
            <p>
              Existing benchmarks evaluate specific capabilities and modalities. There does not exist a truly generalist benchmark
              which is a one-stop shop for a holistic evaluation of a given model - evaluating action capabilities of SOTA VLMs, and
              Multimodal understanding and generation capabilities of SOTA VLAs. A benchmark like this would require effort
              researching, evaluating, and consolidating a wide variety of high quality, relevant, and diverse set of datasets, tasks, and
              metrics. This is what Multinet aims to bring to the field of AI. In this work, we extensively researched the metrics that
              can be used to evaluate the capabilities of these SOTA models on a variety of tasks across vision-language, language,
              and control tasks.
            </p>
            <table>
              <thead>
                <tr>
                  <th>Metric</th>
                  <th>Evaluation Category</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>CIDEr</td>
                  <td>Image Captioning, Image-based Text retrieval</td>
                </tr>
                <tr>
                  <td>VQA Accuracy</td>
                  <td>Visual Question Answering</td>
                </tr>
                <tr>
                  <td>Recall@K</td>
                  <td>Image understanding, Text-based image retrieval</td>
                </tr>
                <tr>
                  <td>Accuracy</td>
                  <td>VQA, Commonsense reasoning, Text understanding</td>
                </tr>
                <tr>
                  <td>Mean Squared Error</td>
                  <td>RL, Robotics</td>
                </tr>
              </tbody>
            </table>
            
            <p style="text-align: center; font-weight: bold;">Metrics in Multinet benchmark and the categories of tasks they each evaluate</p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          
        </div>
      </div>
    </div>
  </div>
</footer>



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
