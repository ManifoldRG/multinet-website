<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Multinet</title>
  <link rel="icon" type="image/x-icon" href="static/images/Multinetlogo.png"> 
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

</head>
<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://manifoldrg.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Related Projects
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/eihli/mugato">
            Î¼Gato
          </a>
          <a class="navbar-item" href="https://github.com/ManifoldRG/MultiNet/tree/main/src/modules">
            GenESIS Framework
          </a>
          <a class="navbar-item" href="https://multinet.ai/static/pages/Multinetv01.html">
            Multinet v0.1
          </a>
        </div>
        
      </div>
    </div>

  </div>
</nav>



  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MultiNet: A Generalist Benchmark for Vision-Language & Action models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">This is a collaborative effort between <a href="https://www.manifoldrg.com/">Manifold Research</a> and <a href="https://metarch.ai/">Metarch.ai</a></span>
              <span class="author-block">This work is sponsored by <a href="https://metarch.ai/">Metarch.ai</a></span>
            </div>

            <div class="publication-links">
              <!-- Multinet v0.1 page -->
              <span class="link-block">
                <a href="static/pages/Multinetv01.html" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-robot"></i>
                  </span>
                  <span>Multinetv0.1</span>
                </a>
              </span>

              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="static/pdfs/Benchmarking Vision Language Action Models.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary PDF link -->
              <span class="link-block">
                <a href="static/pdfs/MultiNet Dataset Spec Paper.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Dataset Spec</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/YOUR REPO HERE" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- Link to VLM-Action framework -->
              <span class="link-block">
                <a href="https://github.com/ManifoldRG/MultiNet/tree/main/src/modules" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-code"></i>
                  </span>
                  <span>GenESIS framework</span>
                </a>
              </span>

              <!-- Link to VLM-Action framework -->
              <span class="link-block">
                <a href="https://github.com/eihli/mugato" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-code"></i>
                  </span>
                  <span>Î¼Gato</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Introduction</h2>
            <p>
              Introducing MultiNet: a comprehensive benchmark we are building to evaluate Vision-Language and Multimodal Action models on their generalist capabilities. With Multinet, we aim to provide:
            </p>
            <ul>
              <li>The largest collection of open-source datasets for evaluating Vision-Language and Multimodal Action models</li>
              <li>Open-source software infrastructure for downloading, managing, and utilizing the datasets mentioned above</li>
              <li>Open-source code to translate RL and Robotics datasets of various formats from various sources into a unified Tensorflow Datasets format</li>
              <li>A general framework for mapping VLMs to other modality classes, with particular emphasis on action spaces</li>
              <li>Open-source frameworks to evaluate SoTA VLMs and VLAs on datasets within Multinet</li>
            </ul>
            <p>
              In our <a href="static/pages/Multinetv01.html">initial release</a>, we present the results of evaluating GPT-4o (SOTA VLM), OpenVLA (SOTA VLA), and HuggingFace's JAT (SOTA generalist model) on 20 OpenX datasets. We aim to quickly iterate and expand our evaluation results to more datasets within Multinet.
            </p>
            <p>
              <strong><em>We are always excited about quick and longer term collaborators getting involved! So, if you are interested in contributing please reach out to us via <a href="https://discord.gg/Rk4gAq5aYr">Discord</a></em></strong>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <style>
    .news-section {
      padding: 20px 0;
      max-height: 400px;
      overflow-y: auto;
    }
  
    .news-title {
      font-size: 24px;
      font-weight: bold;
      margin-bottom: 10px;
    }
  
    .news-container {
      display: flex;
      flex-direction: column;
      gap: 10px;
      background-color: #f5f5f5;
      padding: 20px;
      border-radius: 5px;
    }
  
    .news-item {
      display: flex;
      align-items: center;
      padding: 10px;
      background-color: white;
      border-radius: 5px;
      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
  
    .news-icon {
      font-size: 24px;
      margin-right: 10px;
    }
  
    .news-content {
      flex-grow: 1;
    }
  
    .news-content a {
      color: blue;
      text-decoration: none;
    }
  
    .news-content a:hover {
      text-decoration: underline;
    }
  </style>

  <section class="news-section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">News</h2>
            <div class="news-container">
              <div class="news-item">
                <div class="news-icon">ðŸŽ‰</div>
                <div class="news-content">
                  <p><strong>Multinet v0.1!</strong> How generalist are today's VLMs and VLAs? Read more on our <a href="static/pages/Multinetv01.html">Release page</a></p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Motivation</h2>
            <ul>
              <li>Deepmind's <a href="https://deepmind.google/discover/blog/a-generalist-agent/">GATO</a> showed the first glimpse of what a generalist model could look like, and the capabilities it could unlock.
                <ul>
                  <li>Dataset composition - Gato is trained on a large number of datasets comprising agent experience in both simulated and real world environments, as well as a variety of natural language and image datasets. However, most of GATO's datasets are internal and closed-source, and the model was never released - both OS or API</li>
                </ul> 
              <li>The emerging class of Vision-Language-Action models (<a href="https://openvla.github.io/">OpenVLA</a>, <a href="https://octo-models.github.io/">Octo</a>) display interesting capabilities of taking vision-language-grounded action. However, they are trained for a niche set of robotics tasks, very nascent, and might not be great at challenging vision-language understanding/generation tasks</li>
              <li>In order to truly build something generalist that can work on a myriad of tasks, in a variety of environments, a model/system needs to be trained on a combination of multiple modalities. It should have the ability to understand vision-language input, and generate meaningful outputs. It should have the ability to generate actions for different tasks in a variety of environments where the observation and action spaces can either be continuous or discrete. This will be vision-language-action in the truest sense - good at each of these modalities, as well as at tasks that involve combinations of these modalities - this is what typically occurs in the real world. </li>
            </ul>
            <p>
              There doesn't exist a large, truly open-source, generalist dataset that can be used for pre-training, fine-tuning, and eval. This is the main reason the Multinet effort started.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  

  <style>
    table {
      width: 100%;
      border-collapse: collapse;
      font-family: "Google Sans", "Noto Sans", "Castoro", sans-serif;
    }
    
    th, td {
      padding: 12px 16px;
      text-align: left;
      border-bottom: 1px solid #ddd;
    }
    
    th {
      background-color: #f2f2f2;
      font-weight: bold;
    }
    
    tr:hover {
      background-color: #f5f5f5;
    }
    
    caption {
      font-style: italic;
      margin-top: 8px;
    }
  </style>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-justified">
          <h2 class="title is-3 has-text-centered">Dataset Coverage and Analysis</h2>
          <p>
            Multinet brings a diverse set of datasets which can be used to train models that will have high vision-language 
            association knowledge, language understanding and generation abilities, and prowess in reward-based action
            trajectories in a variety of environments and tasks. Additionally, it can be used to evaluate SOTA VLMs and
            VLAs to showcase where they are lacking and what work needs to be done to build the next generation of truly
            generalist models.
          </p>

            <table>
              <thead>
                <tr>
                  <th style="text-align: center;">Dataset</th>
                  <th style="text-align: center;">Task/Data type</th>
                  <th style="text-align: center;">Modality</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align: center;">OBELICS</td>
                  <td style="text-align: center;">Interleaved Image-Text</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">COYO-700M</td>
                  <td style="text-align: center;">Image-Text pairs</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">MS COCO</td>
                  <td style="text-align: center;">Object detection, segmentation, key-point detection, captioning</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">Conceptual Captions</td>
                  <td style="text-align: center;">Image Captioning</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">A-OKVQA</td>
                  <td style="text-align: center;">Visual Question Answering</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">VQA-v2</td>
                  <td style="text-align: center;">Visual Question Answering</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">Datacomp-1B</td>
                  <td style="text-align: center;">Image-Text pairs</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">Fineweb-edu</td>
                  <td style="text-align: center;">High quality text corpus</td>
                  <td style="text-align: center;">Language</td>
                </tr>
              </tbody>
            </table>
            
            <p style="text-align: center; font-weight: bold; margin-top: 3rem;">Categorization of Vision-Language and Language datasets that can be used for training, fine-tuning, and some of them for evaluation</p>
        </div>

            
            <table>
              <thead>
                <tr>
                  <th style="text-align: center;">Dataset</th>
                  <th style="text-align: center;">Task/Data type</th>
                  <th style="text-align: center;">Modality</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align: center;">Flickr30k</td>
                  <td style="text-align: center;">Image Captioning</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">TextVQA</td>
                  <td style="text-align: center;">Visual Question Answering</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">VizWiz</td>
                  <td style="text-align: center;">Visual Question Answering</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">WinoGAViL</td>
                  <td style="text-align: center;">Vision-based Commonsense Reasoning</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">ImageNet-R</td>
                  <td style="text-align: center;">Image-Text Pairs</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">ObjectNet</td>
                  <td style="text-align: center;">Image-Text Pairs</td>
                  <td style="text-align: center;">Vision-Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">Hellaswag</td>
                  <td style="text-align: center;">Commonsense Reasoning</td>
                  <td style="text-align: center;">Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">ARC</td>
                  <td style="text-align: center;">Complex Reasoning and Knowledge Application</td>
                  <td style="text-align: center;">Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">CommonsenseQA</td>
                  <td style="text-align: center;">Commonsense Reasoning</td>
                  <td style="text-align: center;">Language</td>
                </tr>
                <tr>
                  <td style="text-align: center;">MMLU</td>
                  <td style="text-align: center;">Knowledge-intensive Question Answering</td>
                  <td style="text-align: center;">Language</td>
                </tr>
              </tbody>
            </table>

            <p style="text-align: center; font-weight: bold; margin-bottom: 3rem;">Categorization of Vision-Language and Language datasets that are purely for evaluation</p>


            <table>
              <thead>
                <tr>
                  <th style="text-align: center;">Dataset</th>
                  <th style="text-align: center;">Task/Data type</th>
                  <th style="text-align: center;">Control Category</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align: center;">DM Lab</td>
                  <td style="text-align: center;">Teach RL Agents 3D vision</td>
                  <td style="text-align: center;">Navigation-based control</td>
                </tr>
                <tr>
                  <td style="text-align: center;">DM Control Suite</td>
                  <td style="text-align: center;">Physics-based simulation environments</td>
                  <td style="text-align: center;">Locomotion-based control</td>
                </tr>
                <tr>
                  <td style="text-align: center;">ALE Atari</td>
                  <td style="text-align: center;">Atari games</td>
                  <td style="text-align: center;">Game-based control</td>
                </tr>
                <tr>
                  <td style="text-align: center;">Baby AI</td>
                  <td style="text-align: center;">Language-grounded navigation</td>
                  <td style="text-align: center;">Navigation-based control</td>
                </tr>
                <tr>
                  <td style="text-align: center;">MuJoCo</td>
                  <td style="text-align: center;">Multi-joint dynamics</td>
                  <td style="text-align: center;">Locomotion-based control</td>
                </tr>
                <tr>
                  <td style="text-align: center;">V-D4RL</td>
                  <td style="text-align: center;">Pixel-based analogues of DM Control Suite</td>
                  <td style="text-align: center;">Locomotion-based control</td>
                </tr>
                <tr>
                  <td style="text-align: center;">Procgen</td>
                  <td style="text-align: center;">Procedurally generated Atari-like environments</td>
                  <td style="text-align: center;">Game-based control</td>
                </tr>
                <tr>
                  <td style="text-align: center;">Open X Embodiment</td>
                  <td style="text-align: center;">Real-world Robotics tasks</td>
                  <td style="text-align: center;">Manipulation-based control and Locomotion-based control</td>
                </tr>
                <tr>
                  <td style="text-align: center;">LocoMuJoCo</td>
                  <td style="text-align: center;">Imitation learning for locomotion</td>
                  <td style="text-align: center;">Locomotion-based control</td>
                </tr>
              </tbody>
            </table>
            
            <p style="text-align: center; font-weight: bold; margin-top: 1rem;">Categorization of control datasets</p>


            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 800 400" style="width: 100%; max-width: 800px; height: auto; margin: 20px auto; display: block;">
              <!-- Title -->
              <text x="400" y="30" text-anchor="middle" font-size="20" font-family="Google Sans">Distribution of datasets across modalities in Multinet</text>
              
              <!-- Pie Chart -->
              <g transform="translate(250, 200)">
                  <!-- Control segment (58%) -->
                  <path d="M 0 0 L 100 0 A 100 100 0 1 1 -78.5 -62 Z" 
                        fill="#FFEB99" />
                  
                  <!-- Vision-Language segment (29%) -->
                  <path d="M 0 0 L -78.5 -62 A 100 100 0 0 1 48.5 -87.5 Z" 
                        fill="#6666FF" />
                  
                  <!-- Language segment (13%) -->
                  <path d="M 0 0 L 48.5 -87.5 A 100 100 0 0 1 100 0 Z" 
                        fill="#66B2FF" />
              </g>
              
              <!-- Legend -->
              <g transform="translate(500, 150)">
                  <!-- Vision-Language -->
                  <rect x="0" y="0" width="20" height="20" fill="#6666FF"/>
                  <text x="30" y="15" font-size="16" font-family="Google Sans">Vision-Language (29%)</text>
                  
                  <!-- Language -->
                  <rect x="0" y="35" width="20" height="20" fill="#66B2FF"/>
                  <text x="30" y="50" font-size="16" font-family="Google Sans">Language (13%)</text>
                  
                  <!-- Control -->
                  <rect x="0" y="70" width="20" height="20" fill="#FFEB99"/>
                  <text x="30" y="85" font-size="16" font-family="Google Sans">Control (58%)</text>
              </g>
              
              <!-- Caption -->
              <text x="400" y="350" text-anchor="middle" font-size="14" font-family="Google Sans" font-style="italic">
                  <tspan x="400" dy="0">Control represents the largest portion (58%) due to the extensive OpenX-Embodiment collection,</tspan>
                  <tspan x="400" dy="20">followed by Vision-Language (29%) and Language (13%) datasets.</tspan>
              </text>
          </svg>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Benchmark</h2>
            <p>
              Existing benchmarks evaluate specific capabilities and modalities. There does not exist a truly generalist benchmark
              which is a one-stop shop for a holistic evaluation of a given model - evaluating action capabilities of SOTA VLMs, and
              Multimodal understanding and generation capabilities of SOTA VLAs. A benchmark like this would require effort
              researching, evaluating, and consolidating a wide variety of high quality, relevant, and diverse set of datasets, tasks, and
              metrics. This is what Multinet aims to bring to the field of AI. In this work, we extensively researched the metrics that
              can be used to evaluate the capabilities of these SOTA models on a variety of tasks across vision-language, language,
              and control tasks.
            </p>
            <table>
              <thead>
                <tr>
                  <th style="text-align: center;">Metric</th>
                  <th style="text-align: center;">Evaluation Category</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align: center;">CIDEr</td>
                  <td style="text-align: center;">Image Captioning, Image-based Text retrieval</td>
                </tr>
                <tr>
                  <td style="text-align: center;">VQA Accuracy</td>
                  <td style="text-align: center;">Visual Question Answering</td>
                </tr>
                <tr>
                  <td style="text-align: center;">Recall@K</td>
                  <td style="text-align: center;">Image understanding, Text-based image retrieval</td>
                </tr>
                <tr>
                  <td style="text-align: center;">Accuracy</td>
                  <td style="text-align: center;">VQA, Commonsense reasoning, Text understanding</td>
                </tr>
                <tr>
                  <td style="text-align: center;">Mean Squared Error</td>
                  <td style="text-align: center;">RL, Robotics</td>
                </tr>
              </tbody>
            </table>
            
            <p style="text-align: center; font-weight: bold;">Metrics in Multinet benchmark and the categories of tasks they each evaluate</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Importance of Multinet</h2>
            <p>
              Multinet is the first step in paving the path towards a new class of foundation models that are truly generalist. Multinet aims to showcase the
              gap in the cutting-edge of the field, for models that work across multiple modalities, tasks, and environments.
              In our <a href="static/pages/Multinetv01.html">first release</a> we show the initial version of this - today's SOTA VLMs and VLAs are not
              each impressive and reliable on Vision-Language, Language, RL, and Robotics tasks.
              With datasets such as the ones included in Multinet, and benchmarks to evaluate performance, better VLAs 
              can be built, that are truly Vision-Language-Action models in the sense that they are SOTA in each of these
              modalities, and not just Vision-Language-grounded Actions. Multinet includes pre-training scale data across
              these modalities, and will only grow in size in the future versions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">What's Next?</h2>
            <p>
              <p>This is just the beginning of a new class of foundation models, training data, evaluation methods, and
              benchmarks. We hope to make Multinet a much bigger and useful project with our future versions.</p>
              <ul>
                <li>In future versions we will be profiling the performance of SOTA VLAs on pure Vision-Language and Language
                tasks to assess whether their fine-tuning or co-training on control tasks reduces their capabilities in tasks
                related to each of the individual modalities.</li>
                <li>We also plan to go beyond the OpenX Embodiment dataset, and evaluate the models on other control tasks in Multinet to
                understand the performance of VLAs and generalist models on data that is completely out-of-distribution.</li>
                <li>Currently, our profiling efforts as seen in our <a href="static/pages/Multinetv01.html">first release</a> are completely zero-shot, but we plan to also assess the capabilities of these
                models when they are evaluated few-shot, and fine-tuned.</li>
                <li>We are especially interested in fine-tuning and transferring VLAs to environments that are very different but
                analogous to embodied environments - such as software environments, in order to build digital agents.</li>
                <li>Currently, Multinet is an offline benchmark, but in the future we will be exploring methods to make it an
                online benchmark which may include simulation environments for 2D and 3D control tasks.</li>
              </ul>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX"></section>
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{park2021nerfies,
    author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
    title     = {Nerfies: Deformable Neural Radiance Fields},
    journal   = {ICCV},
    year      = {2021},
  }</code></pre>
    </div>
  </section>



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          
        </div>
      </div>
    </div>
  </div>
</footer>



<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
