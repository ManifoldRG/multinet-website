<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MultinetX-v0.1</title>
  
  <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
  <link rel="stylesheet" href="../css/bulma.min.css">
  <link rel="stylesheet" href="../css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../css/bulma-slider.min.css">
  <link rel="stylesheet" href="../css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="../js/fontawesome.all.min.js"></script>
  <script src="../js/bulma-carousel.min.js"></script>
  <script src="../js/bulma-slider.min.js"></script>
  <script src="../js/index.js"></script>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Benchmarking Vision, Language, & Action Models On
                Robotic Learning Tasks</h1>
                <div class="is-size-5 publication-authors">
                    <!-- Paper authors -->
                    <span class="author-block">
                      <a href="https://www.linkedin.com/in/pranav-guruprasad-82697514a/" target="_blank">Pranav Guruprasad</a><sup>*12</sup>,</span>
                      <span class="author-block">
                        <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Harsh Sikka</a><sup>*123</sup>,</span>
                        <span class="author-block">
                          <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jaewoo Song</a><sup>*1</sup>,</span>
                          <span class="author-block">
                            <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Yangyue Wang</a><sup>1</sup>,</span>
                            <span class="author-block">
                              <a href="FIFTH AUTHOR PERSONAL LINK" target="_blank">Paul Pu Liang</a><sup>4</sup>,</span>
                        </span>
                        </div>
      
                        <div class="is-size-5 publication-authors">
                          <span class="author-block"><a href="https://www.manifoldrg.com/">Manifold Research</a><sup>1</sup>, <a href="https://metarch.ai/">Metarch.ai</a><sup>2</sup>, Georgia Institute of Technology<sup>3</sup>, MIT<sup>4</sup><br>This work is sponsored by <a href="https://metarch.ai/">Metarch.ai</a></span>
                          <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                 <!-- Link to paper -->
                              <span class="link-block">
                                <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                                class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                  <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>Paper</span>
                              </a>
                            </span>
        
                          <!-- Github link -->
                          <span class="link-block">
                            <a href="https://github.com/YOUR REPO HERE" target="_blank"
                            class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                              <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                          </a>
                        </span>
        
                        <!-- Link to VLM-Action framework -->
                        <span class="link-block">
                          <a href="https://github.com/ManifoldRG/MultiNet/tree/main/src/modules" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fab fa-code"></i>
                          </span>
                          <span>GenESIS framework</span>
                        </a>
                      </span>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Abstract</h2>
            <p>
                Vision-language-action (VLA) models represent a promising direction for developing general-purpose
                robotic systems, demonstrating the ability to combine visual understanding, language comprehension,
                and action generation. However, systematic evaluation of these models across diverse robotic tasks
                remains limited. In this work, we present a comprehensive evaluation framework and benchmark suite
                for assessing VLA models. We profile three state-of-the-art VLM and VLAs —GPT-4o, OpenVLA,
                and JAT—across 20 diverse datasets from the Open-X-Embodiment collection, evaluating their
                performance on various manipulation tasks. Our analysis reveals several key insights: (1) current
                VLA models show significant variation in performance across different tasks and robot platforms, with
                GPT-4o demonstrating the most consistent performance through sophisticated prompt engineering,
                (2) all models struggle with complex manipulation tasks requiring multi-step planning, and (3) model
                performance is notably sensitive to action space characteristics and environmental factors. We release
                our evaluation framework and findings to facilitate systematic assessment of future VLA models and
                identify critical areas for improvement in the development of general-purpose robotic systems.
            
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>



</body>
</html>

