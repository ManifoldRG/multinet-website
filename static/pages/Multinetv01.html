<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multinetv0.1</title>
  
  <link rel="icon" type="image/x-icon" href="../images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
  <link rel="stylesheet" href="../css/bulma.min.css">
  <link rel="stylesheet" href="../css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../css/bulma-slider.min.css">
  <link rel="stylesheet" href="../css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="../js/fontawesome.all.min.js"></script>
  <script src="../js/bulma-carousel.min.js"></script>
  <script src="../js/bulma-slider.min.js"></script>
  <script src="../js/index.js"></script>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Benchmarking Vision, Language, & Action Models On
                Robotic Learning Tasks</h1>
                <div class="is-size-5 publication-authors">
                    <!-- Paper authors -->
                    <span class="author-block">
                      <a href="https://www.linkedin.com/in/pranav-guruprasad-82697514a/" target="_blank">Pranav Guruprasad</a><sup>*12</sup>,</span>
                      <span class="author-block">
                        <a href="https://www.harshsikka.com/" target="_blank">Harsh Sikka</a><sup>*123</sup>,</span>
                        <span class="author-block">
                          <a href="https://songstudio.info/" target="_blank">Jaewoo Song</a><sup>*1</sup>,</span>
                          <span class="author-block">
                            <a href="https://www.linkedin.com/in/yangyue-wang" target="_blank">Yangyue Wang</a><sup>1</sup>,</span>
                            <span class="author-block">
                              <a href="https://pliang279.github.io/" target="_blank">Paul Pu Liang</a><sup>4</sup>,</span>
                        </span>
                        </div>
      
                        <div class="is-size-5 publication-authors">
                          <span class="author-block"><a href="https://www.manifoldrg.com/">Manifold Research</a><sup>1</sup>, <a href="https://metarch.ai/">Metarch.ai</a><sup>2</sup>, Georgia Institute of Technology<sup>3</sup>, MIT<sup>4</sup><br>This work is sponsored by <a href="https://metarch.ai/">Metarch.ai</a></span>
                          <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                 <!-- Link to paper -->
                              <span class="link-block">
                                <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                                class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                  <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>Paper</span>
                              </a>
                            </span>
        
                          <!-- Github link -->
                          <span class="link-block">
                            <a href="https://github.com/YOUR REPO HERE" target="_blank"
                            class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                              <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                          </a>
                        </span>
        
                        <!-- Link to VLM-Action framework -->
                        <span class="link-block">
                          <a href="https://github.com/ManifoldRG/MultiNet/tree/main/src/modules" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fas fa-code"></i>
                          </span>
                          <span>GenESIS framework</span>
                        </a>
                      </span>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Abstract</h2>
            <p>
                Vision-language-action (VLA) models represent a promising direction for developing general-purpose
                robotic systems, demonstrating the ability to combine visual understanding, language comprehension,
                and action generation. However, systematic evaluation of these models across diverse robotic tasks
                remains limited. In this work, we present a comprehensive evaluation framework and benchmark suite
                for assessing VLA models. We profile three state-of-the-art VLM and VLAs —GPT-4o, OpenVLA,
                and JAT—across 20 diverse datasets from the Open-X-Embodiment collection, evaluating their
                performance on various manipulation tasks. Our analysis reveals several key insights: (1) current
                VLA models show significant variation in performance across different tasks and robot platforms, with
                GPT-4o demonstrating the most consistent performance through sophisticated prompt engineering,
                (2) all models struggle with complex manipulation tasks requiring multi-step planning, and (3) model
                performance is notably sensitive to action space characteristics and environmental factors. We release
                our evaluation framework and findings to facilitate systematic assessment of future VLA models and
                identify critical areas for improvement in the development of general-purpose robotic systems.
            
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Dataset coverage</h2>
            <p>
              All 3 SoTA models - HuggingFace's JAT (generalist model, OS implementation of GATO), GPT-4o (VLM), and OpenVLA (VLA) were evaluated across the 20 diverse datasets from the Open-X-Embodiment collection listed below, evaluating their
              performance on various manipulation tasks. JAT was also evaluated on 33 other OpenX Embodiment datasets, and GPT-4o was evaluated on 2 more OpenX Embodiment datasets. You can find the results of these evaluations in the <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf">paper</a>
            </p>
            <style>
          .table-container {
              max-width: 100%;
              margin: 20px auto;
              overflow-x: auto;
              font-family: "Google Sans", "Noto Sans", "Castoro", sans-serif;
          }
          
          table {
              border-collapse: collapse;
              width: 100%;
              margin-bottom: 10px;
          }
          
          th, td {
              padding: 12px;
              text-align: left;
              border-bottom: 1px solid #ddd;
          }
          
          th {
              background-color: #f5f5f5;
              font-weight: bold;
          }
          
          tr:hover {
              background-color: #f9f9f9;
          }
          
          .caption {
              font-size: 1.2em;
              font-weight: bold;
              margin-bottom: 15px;
              text-align: center;
          }
          
          .table-notes {
              font-size: 0.9em;
              color: #666;
              text-align: center;
              margin-top: 10px;
              font-style: italic;
          }
          
          .checkmark {
              color: green;
              font-weight: bold;
          }
      </style>
      <div class="table-container">
          <div class="caption">Dataset Coverage and Action Space Characteristics</div>
          
          <table>
              <thead>
                  <tr>
                      <th>Dataset Name</th>
                      <th>Registered Dataset Name</th>
                      <th>In Pretraining for OpenVLA</th>
                      <th>Action Space Type</th>
                  </tr>
              </thead>
              <tbody>
                  <tr>
                      <td>Jaco Play</td>
                      <td>jaco_play</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>4D (1 grip, 3 pos)</td>
                  </tr>
                  <tr>
                      <td>Berkeley Cable Routing</td>
                      <td>berkeley_cable_routing</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>7D (3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>NYU Door Opening</td>
                      <td>nyu_door_opening_surprising_effectiveness</td>
                      <td></td>
                      <td>8D (1 grip, 3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>VIOLA</td>
                      <td>viola</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>8D (1 grip, 3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>Berkeley Autolab UR5</td>
                      <td>berkeley_autolab_ur5</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>8D (1 grip, 3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>TOTO</td>
                      <td>toto</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>7D (3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>Columbia PushT</td>
                      <td>columbia_cairlab_pusht_real</td>
                      <td></td>
                      <td>8D (1 grip, 3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>NYU ROT</td>
                      <td>nyu_rot_dataset_converted_externally_to_rlds</td>
                      <td></td>
                      <td>7D (3 pos, 3 ang, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>Stanford HYDRA</td>
                      <td>stanford_hydra_dataset_converted_externally_to_rlds</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>7D (3 pos, 3 ang, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>UCSD Kitchen</td>
                      <td>ucsd_kitchen_dataset_converted_externally_to_rlds</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>8D (3 pos, 3 ang, 1 grip, 1 term)</td>
                  </tr>
                  <tr>
                      <td>UCSD Pick Place</td>
                      <td>ucsd_pick_and_place_dataset_converted_externally_to_rlds</td>
                      <td></td>
                      <td>4D (3 vel, 1 grip torque)</td>
                  </tr>
                  <tr>
                      <td>USC Cloth Sim</td>
                      <td>usc_cloth_sim_converted_externally_to_rlds</td>
                      <td></td>
                      <td>4D (3 pos, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>Tokyo PR2 Fridge</td>
                      <td>utokyo_pr2_opening_fridge_converted_externally_to_rlds</td>
                      <td></td>
                      <td>8D (3 pos, 3 ang, 1 grip, 1 term)</td>
                  </tr>
                  <tr>
                      <td>Tokyo PR2 Tabletop</td>
                      <td>utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds</td>
                      <td></td>
                      <td>8D (3 pos, 3 ang, 1 grip, 1 term)</td>
                  </tr>
                  <tr>
                      <td>UTokyo xArm Pick-Place</td>
                      <td>utokyo_xarm_pick_and_place_converted_externally_to_rlds</td>
                      <td></td>
                      <td>7D (3 pos, 3 ang, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>Stanford MaskVIT</td>
                      <td>stanford_mask_vit_converted_externally_to_rlds</td>
                      <td></td>
                      <td>5D (3 pos, 1 ang, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>ETH Agent Affordances</td>
                      <td>eth_agent_affordances</td>
                      <td></td>
                      <td>6D (3 vel, 3 ang vel)</td>
                  </tr>
                  <tr>
                      <td>Imperial Sawyer</td>
                      <td>imperialcollege_sawyer_wrist_cam</td>
                      <td></td>
                      <td>8D (3 pos, 3 ang, 1 grip, 1 term)</td>
                  </tr>
                  <tr>
                      <td>ConqHose</td>
                      <td>conq_hose_manipulation</td>
                      <td></td>
                      <td>7D (3 pos, 3 ang, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>Plex RoboSuite</td>
                      <td>plex_robosuite</td>
                      <td></td>
                      <td>7D (3 pos, 3 ang, 1 grip)</td>
                  </tr>
              </tbody>
          </table>
          
          <div class="table-notes">
              pos: position, grip: gripper, term: terminate, vel: velocity, ang: angular
          </div>
      </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Benchmark</h2>
            <p>
                We benchmark a SoTA VLM (GPT-4o), a SoTA VLA (OpenVLA), and a SoTA generalist model (JAT) across 20 diverse datasets from the Open-X-Embodiment collection. 
                In order to evaluate each of these models on offline Robotics trajectories, we utilize Mean Squared Error (MSE) as a metric to compare the predicted action of the model and the ground truth action from the dataset on a per-timestep basis.
                The MSEs per timestep are averaged over all the timesteps in the eval split of the dataset to give an Averaged Mean Squared Error (AMSE) for the whole dataset. This metric is used to compare the performance of the different models given a dataset.
                In order to account for the difference in the scales of the action spaces (and therefore, the MSEs) across datasets, we normalize the per-timestep MSEs using the minimum and maximum MSE values observed in the evaluation (timestep MSE - min MSE) / (max MSE - min MSE). This is then averaged over the entire dataset to give a Normalized Average Mean Squared Error (NAMSE) for each dataset.
                The NAMSE is a metric that allows comparison of a given model's performance across different datasets.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

</body>
</html>

