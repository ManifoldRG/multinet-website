<!DOCTYPE html>
<html>
<head>
    <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PP82HPYSRH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PP82HPYSRH');
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multinetv0.2</title>
  
  <link rel="icon" type="image/x-icon" href="../images/Multinetlogo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
  <link rel="stylesheet" href="../css/bulma.min.css">
  <link rel="stylesheet" href="../css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../css/bulma-slider.min.css">
  <link rel="stylesheet" href="../css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="../js/fontawesome.all.min.js"></script>
  <script src="../js/bulma-carousel.min.js"></script>
  <script src="../js/bulma-slider.min.js"></script>
  <script src="../js/index.js"></script>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Benchmarking Vision, Language, & Action Models in Procedurally
                Generated, Open Ended Action Environments</h1>
                <div class="is-size-5 publication-authors">
                    <!-- Paper authors -->
                    <span class="author-block">
                      <a href="https://www.linkedin.com/in/pranav-guruprasad-82697514a/" target="_blank">Pranav Guruprasad</a><sup>*12</sup>,</span>
                        <span class="author-block">
                            <a href="https://www.linkedin.com/in/yangyue-wang" target="_blank">Yangyue Wang</a><sup>*12</sup>,</span>
                        <span class="author-block">
                            <a href="https://www.linkedin.com/in/sudipta-chowdhury-3610a3303/" target="_blank">Sudipta Chowdhury</a><sup>1</sup>,</span>
                        <span class="author-block">
                        <a href="https://www.harshsikka.com/" target="_blank">Harsh Sikka</a><sup>123</sup>,</span>
                        </span>
                        </div>
      
                        <div class="is-size-5 publication-authors">
                          <span class="author-block"><a href="https://www.manifoldrg.com/">Manifold Research</a><sup>1</sup>, <a href="https://metarch.ai/">Metarch.ai</a><sup>2</sup>, Georgia Institute of Technology<sup>3</sup><br>This work is sponsored by <a href="https://metarch.ai/">Metarch.ai.</a> </span>
                          <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                          <br>
                          <span class="author-block"><strong>Interested in evaluating your model on generalist multimodal datasets? Reach out <a href="mailto:contact@metarch.ai"> here</a></strong></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                 <!-- Link to paper -->
                              <span class="link-block">
                                <a href="https://arxiv.org/abs/2505.05540" target="_blank"
                                class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                  <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>Paper</span>
                              </a>
                            </span>
        
                          <!-- Github link -->
                          <span class="link-block">
                            <a href="https://github.com/ManifoldRG/MultiNet" target="_blank"
                            class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                              <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                          </a>
                        </span>
        
                        <!-- Link to VLM-Action framework -->
                        <span class="link-block">
                          <a href="https://github.com/ManifoldRG/MultiNet/tree/main/src/modules" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fas fa-code"></i>
                          </span>
                          <span>GenESIS framework</span>
                        </a>
                      </span>

                        <span class="link-block">
                          <a href="Technicalblog.html" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="fas fa-file-alt"></i>
                          </span>
                          <span>Extracting probabilties from VLAs - Technical Blog</span>
                        </a>
                      </span>
          </div>
        </div>
      </div>
    </div>
  </section>
  <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-centered">
            <img src="../images/Multinet v0.1 release visual 3.0.png" alt="Multinet v0.1 Release Visual" width="85%">
          </div>
        </div>
      </div>
    </div>

  <section class="section"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Abstract</h2>
            <p>
                Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution. However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in procedurally generated out-of-distribution (OOD) environments, remains limited. In this work, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLM and VLA models—including GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, and Pi0 FAST—on diverse procedural tasks from the <a href="https://openai.com/index/procgen-benchmark/" target="_blank">Procgen benchmark</a>. Our analysis reveals several critical insights:
                <ul>
                  <li>All evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexity.</li>
                  <li>VLAs generally outperform other models due to their robust architectural design.</li>
                  <li>VLM variants demonstrate substantial improvements when constrained appropriately, highlighting the sensitivity of model performance to precise prompt engineering.</li>
                </ul>
            
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Dataset coverage</h2>
            <p>
              All 5 models - OpenVLA, Pi0 Base, Pi0 FAST, GPT 4o and GPT 4.1 were evaluated on offline trajectories from expert reinforcement
              learning (RL) agents trained on the Procgen dataset, which were sourced from Facebook’s publicly available repository (https://dl.fbaipublicfiles.com/DGRL/1M/expert/). 
              The test splits for each of the 16 Procgen datasets consisted of randomly sampled 10% of the total episodes in the dataset.  Procgen environments are procedurally generated, Atari-like 2D games 
              designed to evaluate visual and motor skills of RL agents. Each dataset within Procgen exhibits diverse environment layouts, tasks, objectives, reward structures, and discrete actionspaces, 
              predominantly involving directional movements and specific game-related interactions.
            </p>
            <style>
          .table-container {
              max-width: 100%;
              margin: 20px auto;
              overflow-x: auto;
              font-family: "Google Sans", "Noto Sans", "Castoro", sans-serif;
          }
          
          table {
              border-collapse: collapse;
              width: 100%;
              margin-bottom: 10px;
          }
          
          th, td {
              padding: 12px;
              text-align: left;
              border-bottom: 1px solid #ddd;
          }
          
          th {
              background-color: #f5f5f5;
              font-weight: bold;
          }
          
          tr:hover {
              background-color: #f9f9f9;
          }
          
          .caption {
              font-size: 1.2em;
              font-weight: bold;
              margin-bottom: 15px;
              text-align: center;
          }
          
          .table-notes {
              font-size: 0.9em;
              color: #666;
              text-align: center;
              margin-top: 10px;
              font-style: italic;
          }
          
          .checkmark {
              color: green;
              font-weight: bold;
          }
      </style>
      <div class="table-container">
          <div class="caption">Dataset Coverage and Action Space Characteristics</div>
          
          <table>
              <thead>
                  <tr>
                      <th>Dataset Name</th>
                      <th>Registered Dataset Name</th>
                      <th>In Pretraining for OpenVLA</th>
                      <th>Action Space Type</th>
                  </tr>
              </thead>
              <tbody>
                  <tr>
                      <td>Jaco Play</td>
                      <td>jaco_play</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>4D (1 grip, 3 pos)</td>
                  </tr>
                  <tr>
                      <td>Berkeley Cable Routing</td>
                      <td>berkeley_cable_routing</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>7D (3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>NYU Door Opening</td>
                      <td>nyu_door_opening_surprising_effectiveness</td>
                      <td></td>
                      <td>8D (1 grip, 3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>VIOLA</td>
                      <td>viola</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>8D (1 grip, 3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>Berkeley Autolab UR5</td>
                      <td>berkeley_autolab_ur5</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>8D (1 grip, 3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>TOTO</td>
                      <td>toto</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>7D (3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>Columbia PushT</td>
                      <td>columbia_cairlab_pusht_real</td>
                      <td></td>
                      <td>8D (1 grip, 3 ang, 3 pos, 1 term)</td>
                  </tr>
                  <tr>
                      <td>NYU ROT</td>
                      <td>nyu_rot_dataset_converted_externally_to_rlds</td>
                      <td></td>
                      <td>7D (3 pos, 3 ang, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>Stanford HYDRA</td>
                      <td>stanford_hydra_dataset_converted_externally_to_rlds</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>7D (3 pos, 3 ang, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>UCSD Kitchen</td>
                      <td>ucsd_kitchen_dataset_converted_externally_to_rlds</td>
                      <td><span class="checkmark">✓</span></td>
                      <td>8D (3 pos, 3 ang, 1 grip, 1 term)</td>
                  </tr>
                  <tr>
                      <td>UCSD Pick Place</td>
                      <td>ucsd_pick_and_place_dataset_converted_externally_to_rlds</td>
                      <td></td>
                      <td>4D (3 vel, 1 grip torque)</td>
                  </tr>
                  <tr>
                      <td>USC Cloth Sim</td>
                      <td>usc_cloth_sim_converted_externally_to_rlds</td>
                      <td></td>
                      <td>4D (3 pos, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>Tokyo PR2 Fridge</td>
                      <td>utokyo_pr2_opening_fridge_converted_externally_to_rlds</td>
                      <td></td>
                      <td>8D (3 pos, 3 ang, 1 grip, 1 term)</td>
                  </tr>
                  <tr>
                      <td>Tokyo PR2 Tabletop</td>
                      <td>utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds</td>
                      <td></td>
                      <td>8D (3 pos, 3 ang, 1 grip, 1 term)</td>
                  </tr>
                  <tr>
                      <td>UTokyo xArm Pick-Place</td>
                      <td>utokyo_xarm_pick_and_place_converted_externally_to_rlds</td>
                      <td></td>
                      <td>7D (3 pos, 3 ang, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>Stanford MaskVIT</td>
                      <td>stanford_mask_vit_converted_externally_to_rlds</td>
                      <td></td>
                      <td>5D (3 pos, 1 ang, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>ETH Agent Affordances</td>
                      <td>eth_agent_affordances</td>
                      <td></td>
                      <td>6D (3 vel, 3 ang vel)</td>
                  </tr>
                  <tr>
                      <td>Imperial Sawyer</td>
                      <td>imperialcollege_sawyer_wrist_cam</td>
                      <td></td>
                      <td>8D (3 pos, 3 ang, 1 grip, 1 term)</td>
                  </tr>
                  <tr>
                      <td>ConqHose</td>
                      <td>conq_hose_manipulation</td>
                      <td></td>
                      <td>7D (3 pos, 3 ang, 1 grip)</td>
                  </tr>
                  <tr>
                      <td>Plex RoboSuite</td>
                      <td>plex_robosuite</td>
                      <td></td>
                      <td>7D (3 pos, 3 ang, 1 grip)</td>
                  </tr>
              </tbody>
          </table>
          
          <div class="table-notes">
              pos: position, grip: gripper, term: terminate, vel: velocity, ang: angular
          </div>
      </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Results</h2>
            <p>
              insert table here
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Metrics and Benchmark</h2>
            <p>
                We benchmark SoTA VLMs and VLAs on 16 diverse datasets from the Procgen dataset collection. 
                In order to evaluate the performance of each of these models on offline trajectories of expert RL agents in these environments with discrete action spaces on a per-timestep basis, we use a variety of metrics to best capture the performance of the models and assess them as fair as possible.
                The metrics used include - Brier Mean Absolute Error (MAE), Normalized Brier MAE, Normalized Quantile Filtered Brier MAE, Max Relative Brier MAE, Percentage Invalid Actions, and Micro, Macro, and Action Class-wise variations of Precision, Recall, and F1 scores. To visualize the 
                effects of model architecture, training methods, and output processing techniques on the concentrated VS diffused nature of the models' predicted actions, we use confusion matrices depicting the frequency of predictions on a per-action class basis, and the frequency of correct predictions. Finally, to observe the correlation between model performance and image complexity, we use correlation matrices between Shannon Entropy and Delentropy values of 
                the images and the Macro Recall values of the models.

            <p>   
                These metrics address diverse aspects of model performance such as how well calibrated they are in their estimates, reliance on outliers, how they are affected
                by class imbalance, how well they are able to restrict their predictions to the valid action space, whether the models are biased towards certain kinds of subdatasets or actions, etc.
                To understand the metrics better, how they are calculated, and what aspect of the models' performance they capture, we refer the reader to the <a href="https://arxiv.org/abs/2505.05540" target="_blank">paper</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Analysis</h2>
            <p>
                
            </p>   
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Conclusion</h2>
            <p>
                In this study, we systematically evaluated the generalization capabilities of contemporary VLAs and VLMs on procedurally generated discrete-action environments from the Procgen dataset in a zero-shot setting. Our analysis highlighted significant limitations arising from architectural constraints, training paradigms, and input-output biases inherent to the models. The stark domain discrepancy between training data—primarily continuous-action robotics datasets, and general web-scale vision language data—and discrete-action game environments proved to be a critical barrier to effective zero-shot generalization.
                We identified notable differences in model behaviors linked directly to their architectures, training strategies, and input/output processing techniques. OpenVLA's robust action-space clamping technique consistently provided superior generalization, minimizing invalid outputs and exhibiting relative resilience to out-of-distribution scenarios. Conversely, autoregressive models like GPT-4x displayed substantial difficulty in generalizing, especially under complex image conditions, and frequently defaulted to idle or biased action choices. Additionally, Pi0 models showed intermediate performance influenced heavily by their diffusion-based (Pi0 Base) or autoregressive (Pi0 FAST) decoding methods, with Pi0 FAST being notably sensitive to image complexity and unable to restrict the majority of its predictions to a desired output range.
                Our findings underscore the necessity for architectural innovations, refined training methodologies, and enhanced output processing techniques to bridge the gap between diverse action domains. Future research should prioritize developing more generalized training datasets that better reflect the variety of potential application environments, alongside methods to adaptively handle different forms of action representations. These advancements hold promise for enabling VLAs to operate effectively across an increasingly diverse and unpredictable range of real-world tasks.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section"></section>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-justified">
            <h2 class="title is-3 has-text-centered">Acknowledgements</h2>
            <p>
              We thank <a href="https://kpertsch.github.io/">Karl Pertsch</a> for his helpful feedback on this work.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{guruprasad2025benchmarkingvisionlanguage,
        title={Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments}, 
        author={Pranav Guruprasad and Yangyue Wang and Sudipta Chowdhury and Harshvardhan Sikka},
        year={2025},
        eprint={2505.05540},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2505.05540}, 
  }</code></pre>
    </div>
  </section>

  <footer class="footer" style="background-color: #363636;">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content" style="color: #fff;">
            <div class="buttons is-centered mb-4">
              <a href="https://arxiv.org/abs/2505.05540" class="button is-small is-dark" target="_blank">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
              <a href="https://github.com/ManifoldRG/MultiNet" class="button is-small is-dark" target="_blank">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </div>
  
            <p>For questions or issues, please <a href="https://github.com/ManifoldRG/MultiNet/issues">open a GitHub issue</a> or contact <a href="mailto:pranav@metarch.ai">pranav [at] metarch [dot] ai</a></p>

          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>


